<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Deep Learning Final Project | Georgia Tech | Spring 2019: CS 4803/7643</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Self-Monitoring Navigation Agent via Auxiliary Progress Estimation</h1> 
<h2>An Extension with Network Architecture Experiments</h2>
<span style="font-size: 20px; line-height: 1.5em;"><strong>Osvaldo Armas, Daphne Chen, Charles Ramey</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Spring 2019 CS 4803 / 7643 Deep Learning: Final Report</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<h2>Abstract</h2>
<p>Our group replicated and extended the results of the paper: <i>Self-Monitoring Navigation Agent via Auxiliary Progress Estimation</i> (<a href="https://arxiv.org/pdf/1901.03035.pdf">arxiv</a>).
<p>Enabling an autonomous agent to navigate in unknown environments is a known challenge, and many studies have been done on navigation via visual data. This study addresses the problem of an agent dealing with tasks that lack an explicit representation of the goal â€“ for example, determining what an agent should do when it lacks an intermediate action that is a necessary prior for an ongoing task.</p>
<p>The current approach outlined in the paper involves a self-monitoring agent that consists of three modules: visual-textual grounding, progress monitoring, and action selection.</p>
<p>Our work validated the results of the original paper and evaluated two alternative network architectures which were able to reduce training time and augment the original model's textual grounding functionality.</p>
<br>

<div style="text-align: center;">
<img style="height: 200px;" alt="Default" src="img/matterport.png">
</div> <div style="text-align: center;"><b>Figure 1: </b>Example visualized scenes from the dataset</div>

<br>

<h2>Introduction</h2>
<p>Our project aimed to validate the results from "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation" and improve these results by studying the performance differences of two additional network architectures. The existing model that we are studying incorporates three modules which perform visual grounding, textual grounding, and progress monitoring to guide an agent through an environment based off of a given instruction. The visual grounding unit uses ResNet-152 to extract image features, which limits our ability to change that component's architecture [1]. We decided to treat this as a black box and instead focus on improving performance in the action selection and textual grounding units.</p>
<p>We began by setting up our development environments and validating the results from the prior work paper. Once we achieved the baseline performance, we considered ways in which we could either improve the model efficiency or boost validation accuracy (model success rate). Given these motivations, we decided upon using a gated recurrent unit (GRU) in place of an Long Short Term Memory (LSTM) because a GRU has no explicit memory and is thus more efficient [2]. Convolutional layers have formerly been used for sentence classification in order to learn relationships among words, similar to n-grams; thus potentially boosting performance. We expected that each of these modifications would yield different performance than the default LSTM [3].</p>
<p>The prior work utilized LSTM cells in the attention model for instruction parsing as well as in the visual and textual feature co-grounding model. This project sought to examine the performance tradeoffs and advantages of instead using GRU cells and convolutional layers in place of the LSTM cells.</p>
<p>If our project is successful, we will be able to explain the impacts of different network architectures on applied tasks such as self-monitoring navigation. The GRU and convolutional layers, when compared to LSTM cells, will be expected to impact not only performance but also training time. Our results will show how these alternative architectures can be leveraged to improve and extend the prior work. </p>
<br>

<h2>Prior Work</h2>
<p>The original paper designs and implements an agent that can learn to navigate in photo-realistic environments given image data (RGB-D photos of rooms). It is comprised of textual grounding, action selection, progress monitoring, and visual grounding modules.</p>
<p>The follow up work from the same authors of the original paper further examines the use of the model's progress monitor module as a heuristic for evaluating actions when an agent is searching for the best possible next action [4].</p>
<br>

<h2>Applications</h2>
<p><a href="http://ml.gatech.edu/hg/item/620601">The prior work</a> has been highlighted for its ability to better enable autonomous agents to navigate in unknown environments. Deployed on a real robot, this study could be used to replace humans in dangerous, uncertain situations such as rescuing people from burning buildings or driving autonomously. This demonstrates the potential to affect anyone who could benefit from using an autonomous agent in their daily lives in order to augment regular tasks.</p>
<br>

<!-- Approach -->
<h2>Approach</h2>
<!--<h4>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</h4> -->
<p>In order to include GRU cells in place of the the LSTM cells in the textual grounding module as well as the policy model, we made changes to several of the Python files included in the original project. First, the main python file was edited to include a command line parameter which is passable at runtime to enable selection of the GRU cells. Next, the forward pass methods were modified inside of the RNN python file to perform the network's forward pass through GRU cells instead of LSTM cells when the GRU parameter is passed at runtime. Finally, the encoder, panoramic agent, and policy agent python files were modified to use the GRU provided by the RNN file previously modified to include GRU cells.</p>
<p>The motivation behind replacing the baseline model's LSTM cells with GRU cells revolved around the computational efficiency of GRU cells and the difference of the memory mechanism of GRU cells compared to LSTM cells. The original paper noted that different attentional mechanisms may improve model performance. It was our hope that the simpler memory mechanism of GRU cells might provide further information on how memory and attention affect the agent's overall navigational ability.
</p>
<p> The two convolutional layers were added by using (2,1) and (3,1) kernels. This means that the convolutional layers were convolving along 2 and 3 rows respectively. In other words, the convolutional layers were added to convolve along multiple words rather than to convolve along a single word. To maintain simplicity, only one output channel was used. Howevever, future experiments using multiple channels would likely result in more advanced performance. </p>
<p> 
The use of convolutional layers was inspired by n-grams models and how convolutional layers have been previously used to learn relationships among words. The idea was that any learned word relationships would improve attention as each encoding would include information about its surrounding encodings. 
</p>
<p></p>
<p>At the beginning of our project, when conferring with teaching assistants, we were warned about the computational resources that this project might require. The original work was trained and tested on a Titan Xp GPU. In comparison, our group had access to a workstation with a GeForce GTX 980 GPU and a laptop with a GeForce RTX 2070 Max-Q GPU. The logistics of training and testing models became our single largest challenge with this project. On average, it took around 2 days to train and test an experiment. Each time we conducted a comparison experiment with our three models, we required 6 days of training time. This time constraint limited the number of experiments we could conduct within the project time period.</p>
<p>
Most other problems we experienced were related to the complexity of the code base from the original study. The project is a cutting edge system for robotic navigation utilizing multiple collaborative machine learning pipelines. Naturally such a system involves a large amount of data pre-processing  software, software for the machine learning modules, and software for training and testing the machine learning modules. Each modification of the system required a large amount of research and testing to properly locate where modifications needed to be made and how to integrate the modifications to operate seamlessly with the rest of the system. These challenges were most apparent when modifying the network architecture in the textual co-grounding module and collecting custom metrics from the evaluation code. 
</p>

<br>
<h2>Implementation Details</h2>
<p>We used precomputed image features from the <a href="https://niessner.github.io/Matterport/">Matterport dataset</a>, totaling 4.2 GB. Image features were precomputed via ResNet-152 from the ImageNet and Places365 datasets in order to drastically reduce training time. ADAM is used as the optimizer; we chose to stick with this because it is more efficient than stochastic gradient descent (SGD) with low-parameter models such as ours and is specifically designed for training deep networks. In general, ADAM works as a combination between RMSprop and SGD using momentum. The learning rate is 1eâˆ’4 with batch size of 64, which we kept low in order to balance the trade-off of having a high number of iterations (200 iterations over 300 epochs for training) in order to improve model generalization.</p>
<br>
<!-- Results -->
<h2>Experimental Methodology</h2>
<!--<h4>How did you measure success? What datasets did you use and what experiments did you carry out? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</h4> -->
<div style="text-align: center;">
<img style="height: 200px;" alt="Default" src="img/default.png">
<img style="height: 200px;" alt="GRU" src="img/gru.png">
<img style="height: 200px;" alt="Conv" src="img/conv.png">
</div><div style="text-align: center;"><b>Figure 2: </b>top left â€“ default, top right â€“ GRU, bottom â€“ 2-convolutional-layer</div><br>

<p>We first established a baseline by training and testing the model provided with the prior work using the hyperparameters that achieved the highest performance quoted in the paper. Each of the models were implemented using PyTorch and trained on the same machine. We incrementally monitored the training process with Tensorboard. We achieved the same results from the baseline model in the paper.</p>
<p>The following modifications were made to the prior work's model: (1) GRU cells were implemented in place of LSTM cells [5], and (2) two convolutional layers were added in front of the LSTM network with the intent of learning n-grams (bi-grams and tri-grams, respectively for each layer). The architectural changes are pictured above in Figure 2.</p>
<p>Each modified architecture was trained and tested using the same hyperparameters that achieved the highest performance quoted in the paper to enable fair comparisons between the models. Our evaluation metrics for success included the final training loss, highest validation accuracy, navigation error, the agent's distance from goal, total training time, and memory usage. The final results are shown in the following section.</p>
<br>

<h2>Results</h2>
<div style="text-align: center;"><h4>Architecture Modifications</h4></div>
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="img/loss.png">
<img style="height: 200px;" alt="" src="img/val-loss.png">
<img style="height: 200px;" alt="" src="img/dist.png">
<img style="height: 200px;" alt="" src="img/steps.png">
<img style="height: 200px;" alt="" src="img/success-rate.png">
<img style="height: 200px;" alt="" src="img/nav-error.png">
</div>
<div style="text-align: center;"><b>Figure 3: </b>Plots of relevant metrics from training each of the three models.</div>

<div style="display: flex; justify-content: center;">
<img style="height: 50px;" align="middle" alt="" src="img/legend.png">
</div>

<div style="text-align: center;"><h4>Overall Performance</h4></div>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="img/table.png">
</div>
<div style="text-align: center;"><b>Figure 4: </b>A sampling of training results. While the default model demonstrated the best <i>overall</i> performance, the 2-layer Conv1d model was close behind for each of training loss, highest validation accuracy, and navigation error. The GRU model beat the default in total training time, as expected.</div>
<br>

<div style="text-align: center;"><h4>Memory Usage</h4></div>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="img/memory.png">
</div>
<div style="text-align: center;"><b>Figure 5: </b>Measured memory usage in megabytes for each of the three model architectures.</div>
<br>

<div style="text-align: center;"><h4>Instruction Length</h4></div>
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="img/accuracy_instruction_default.png">
<img style="height: 300px;" alt="" src="img/accuracy_instruction_gru.png">
<img style="height: 300px;" altWhat were the results, both quantitative and qualitative="" src="img/accuracy_instruction_conv.png">
</div>
<div style="text-align: center;What were the results, both quantitative and qualitative"><b>Figure 6: </b>Instruction length vs. accuracy for each of the three model architectures.</div>
<br>

<h2>Analysis</h2>

<p>We found that there was little improvement in the overall accuracy of the system, however efficiency (number of steps, training time) did improve. Replacing the LSTM with a GRU decreased the training time by 3.5 hours. This decrease was an 8.5% improvement over the default architecture. Adding 2 convolutional layers as input to the LSTM resulted in similar baseline performance.</p>

<p>In order to provide deeper understanding of model performance, we conducted additional analysis comprised of measuring memory usage and determining accuracy as a function of textual instruction length. The results are shown in Figures 5 and 6 above. Some of these results were unexpected, as we had predicted that higher instruction length would correspond to lower accuracy due to longer instructions being more complex. However, we instead found that there was no strong correlation between instruction length and model accuracy. It could instead be true that longer instructions are not necessarily more complex; rather, particular words and phrases in the vocabulary contribute more towards complexity. Furthermore, each of the models demonstrated significant overfitting, showing that the model was able to comprehensively learn over the given instruction vocabulary.</p>

<p>The 2-convolutional-layer model had significantly higher memory usage than that of the default and GRU, which both had comparable memory usage, with the GRU beating the default model by 7.87 MB. This was expected, since convolutional layers are memory heavy.</p>
<br>

<h2>Future Work</h2>
<p>We would like to guide our progress towards introducing POS (parts of speech) tagging and pre-trained vocabulary or text embeddings in order to see the effect on model performance. This would involve incorporating attended features for POS tags, where there is an assigned probability distribution for different tags. Furthermore, an interesting direction could involve experimenting with additional LSTM layers in the progress monitor and language embedding modules. We would also like to see the effect of implementing a different attentional mechanism, such as a multi-attentional neural network, for textual grounding to improve instruction tracking throughout time.</p>
<p>Generally speaking, we could further extend this study by performing hyperparameter tuning on the default model, but we avoided it for the sake of this project because we judged it to not be a sufficient engineering problem. One consideration is to increase batch size above the constant 64 that we used for each of our experiments, since a higher batch size may improve generalization ability for such a large dataset.</p>
<p>In addition, since we did not make any modifications to the visual grounding unit, future work could include more robust testing such as using different precomputed image features from other image/scene classification datasets such as STL-10 and COIL100. Using different image features may help reduce training times due to ResNet's extensive training cost.</p>
<p>Lastly, since the three of us are all roboticists in practice, we would love to see this work deployed on a real mobile robot. It would be interesting to see whether the added stochasticity in the real world will significantly affect performance. Robots themselves are often unpredictable and difficult to work with, so testing on one would further reinforce the robustness of this study.</p>
<br>
<h2>Team Member Task Breakdown</h2>
<table class="table" style="width:100%">
  <thead>
    <tr>
      <th scope="col">Name</th>
      <th scope="col">Tasks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Osvaldo Armas</td>
      <td>Co-wrote proposal. Set up development environment on the GeForce RTX 2070 Max-Q laptop. Co-wrote poster. Co-wrote final paper. Wrote implementation using convolutional layers. </td>
    </tr>
    <tr>
      <td>Daphne Chen</td>
      <td>Co-wrote proposal. Validated original study results. Made modifications and ran experiments for accuracy versus instruction length data output. Generated Tensorboard plots. Made data analysis tables. Wrote vanilla RNN implementation. Co-wrote poster. Co-wrote final paper.</td>
    </tr>
    <tr>
      <td>Charles Ramey</td>
      <td>Co-wrote proposal. Set up development environment on the GTX 980 Workstation. Validated original study results. Incorporated GRU cells. Tested GRU cells. Trained and evaluated model with GRU cells. Co-wrote poster. Analyzed accuracy results per instruction length. Generated plots of accuracy per instruction length. Co-wrote final paper.</td>
    </tr>
  </tbody>
</table>
<br><br>

<h2>Citations</h2>
<p>[1] C. Ma et al. Self-Monitoring Navigation Agent via Auxiliary Progress Estimation, International Conference on Learning Representations, 2019.</p>
<p>[2] K. Irie et al. LSTM, GRU, Highway and a Bit of Attention: An Empirical Overview for Language Modeling in Speech Recognition, Interspeech, 2016.</p>
<p>[3] Y. Kim. Convolutional Neural Networks for Sentence Classification. Available: https://arxiv.org/abs/1408.5882 </p>
<p>[4] C. Ma et al. The Regretful Agent: Heuristic-Aided Navigation through Progress Estimation, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.</p>
<p>[5] J. Chung. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, NIPS Deep Learning and Representation Learning Workshop, 2014.</p>


<br><br>

  <hr>
  <footer> 
  <p>Â© Osvaldo Armas, Daphne Chen, Chad Ramey</p>
  </footer>




</div>
</div>


</body></html>
