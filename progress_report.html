<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Deep Learning Class Project Progress Check In
  | Georgia Tech | Spring 2019: CS 4803 / 7643</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Self-Monitoring Navigation Agent via Auxiliary Progress Estimation</h1> 
<h2>A Replication Study with Network Architecture Experiments</h2>
<span style="font-size: 20px; line-height: 1.5em;"><strong>Osvaldo Armas, Daphne Chen, Charles Ramey</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Spring 2019 CS 4803 / 7643 Deep Learning: Class Project Progress Check In</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Goal: TODO uncomment this -->
<h2>Abstract</h2>
<p>Our group aims to replicate and extend the results of the study in the paper: <i>Self-Monitoring Navigation Agent via Auxiliary Progress Estimation</i> (<a href="https://arxiv.org/pdf/1901.03035.pdf">arxiv</a>).
<p>Enabling an autonomous agent to navigate in unknown environments is a known challenge, and many studies have been done on navigation via visual data. This study addresses the problem of an agent dealing with tasks that lack an explicit representation of the goal – for example, determining what an agent should do when it lacks an intermediate action that is a necessary prior for an ongoing task.</p>
<p>The current approach outlined in the paper involves a self-monitoring agent that consists of three modules: visual-textual grounding, progress monitoring, and action selection. Our project will aim to complete two main goals: replicate the original study performance using PyTorch, and improve upon the study performance by experimenting with alternative network architectures.</p>

<!-- figure -->
<!--
<h2>Teaser figure</h2>
A figure that conveys the main idea behind the project or the main application being addressed. (This one is from <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a>.)
<br><br>
/* Main Illustrative Figure */ 
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="images/alexnet.png">
</div> -->

<h2>Introduction / Background / Motivation</h2>
<p>Our project aims to validate the results from the existing literature and improve these results by studying the performance differences of different network architectures. Since GRUs have no explicit memory and are more efficient, and vanilla RNNs can remember information for only a few time steps at once, we expect that they will have different performance than the LSTM.</p>
<p>The prior work utilizes Long Short Term Memory cells in the attention model for instruction parsing as well as in the visual and textual feature co-grounding model. This project seeks to examine the performance tradeoffs and advantages of instead using vanilla RNN and GRU cells in place of the LSTM cells.</p>
<p>If our project is successful, we will be able to explain the impacts of different network architectures on applied tasks such as self-monitoring navigation. The vanilla RNN and GRU cells when compared to LSTM cells will be expected to impact not only performance but also training and testing time. Our results will show how these alternative architectures can be leveraged to improve and extend the prior work. </p>

<!-- Approach -->
<h2>Approach</h2>
<!--<h4>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</h4> -->
<p>So far, each member of our project team has individually validated the results of the original study. Once we were able to successfully validate the study results, we began modifying the software to incorporate vanilla RNN and GRU cells. After all modifications are complete, we will run iterations of training and testing to collect data on the differences between network architectures.</p>
<!-- <h4>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</h4>
Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. -->


<!-- Results -->
<h2>Experimental Plan</h2>
<!--<h4>How did you measure success? What datasets did you use and what experiments did you carry out? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</h4> -->
<p>Our experiments will be fairly straightforward: we will train and test the self-monitoring navigation agent with LTSM, vanilla RNN, and GRU cells. We will use TensorBoard to collect data during training and testing of the self-monitoring navigation agent. We will compile the data from all tests and then make comparisons of accuracy, training time, and testing time between the architectures. </p>


<h2>Current Status</h2>
<p>We do not currently have any analysis.</p>
<table style="width:100%">
  <tr>
    <th>Name</th>
    <th>Description of Work</th>
  </tr>
  <tr>
    <td>Osvaldo Armas</td>
    <td>Built docker container for training on GCP GPU instances</td>
  </tr>
  <tr>
    <td>Daphne Chen</td>
    <td>Incorporated RNN cells into code, testing RNN cells</td>
  </tr>
  <tr>
    <td>Charles Ramey</td>
    <td>Incorporated GRU cells into code, testing GRU cells</td>
  </tr>
</table>

<br>

<h2>Help</h2>
<p>So far we have not run in to any problems that require instructor assistance.</p>

<!-- Main Results Figure --> 
<div style="text-align: center;">
<img style="height: 300px;" alt="" src="images/results.png">
</div>
<br><br>

  <hr>
  <footer> 
  <p>© Osvaldo Armas, Daphne Chen, Chad Ramey</p>
  </footer>

<br><br>
<!-- Analysis -->
<!--
<h2>Analysis</h2>
<h4>Do the results make sense? Why or why not? Describe what kind of visualization/analysis you performed in order to verify that your results 1) are correct and 2) explain differences in performance from what was expected (e.g. what appeared in papers). Provide specific claims about why you think your model is or is not doing better, and justify those with qualitative and quantitative experiments (not necessarily just final accuracy numbers, but statistics or other data about what the model is doing).</h4>
Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt.
-->


<br><br>
<!-- Team -->
<!--
<h2>Team Member Identification</h2>
<h4>Provide a list of team members and what each member did in a table</h4>


<table style="width:100%">
  <tr>
    <th>Name</th>
    <th>Description of Work</th>
  </tr>
  <tr>
    <td>Team member 1</td>
    <td>Implemented new loss function, trained and monitored the optimization addressing any issues such as overfitting, tuned hyper-parameters for new loss</td>
  </tr>
  <tr>
    <td>Team member 2</td>
    <td>Designed and implemented attentional mechanism for approach, trying out different approaches such as dot product attention.</td>
  </tr>
</table>
-->

<br><br>
</div>
</div>


</body></html>
